{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ensemble_Learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/namson98/Complete-Python-3-Bootcamp/blob/master/Ensemble_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0tnOpaLBcqT",
        "colab_type": "text"
      },
      "source": [
        "# Ensemble learning\n",
        "\n",
        "Please, make a copy of this colaboratory in order to be able to make changes **(File -> Save a copy in Drive)**.\n",
        "\n",
        "This colaboratory includes practical exercises designed to support theoretical lecture on Ensemble Learning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wptpa5-JBUbu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# For plotting like a pro\n",
        "!pip install plotnine\n",
        "from plotnine import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjqojFDiC6Ib",
        "colab_type": "text"
      },
      "source": [
        "# Basic ensemble via averaging (in the case of regression)\n",
        "\n",
        "As in the lecture slides we shall start by building one the most basic types of ensembles - combining predictions from different models into one. Here we will look into combinding linear regression algorithms (vanila, ridge and lasso). In practice you can use almost any regression model as part of such ensemble. Needless to say that combining predictions produced by the same model on the same data will not result in additional performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACfDV132C9by",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "example_data = pd.DataFrame({'x':[1,2,3,4,5], 'y':[2,4,5,4,5]})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMUQZNRVC2J8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = (\n",
        "    ggplot(data = example_data,\n",
        "          mapping = aes(x = 'x', y = 'y')) +\n",
        "    geom_point(fill = '#36B059', \n",
        "               size = 5.0,\n",
        "               stroke = 2.5,\n",
        "               colour = '#2BE062',\n",
        "               shape = 'o') +\n",
        "    labs(\n",
        "        title ='',\n",
        "        x = 'X',\n",
        "        y = 'y',\n",
        "    ) +\n",
        "    xlim(0, 6) +\n",
        "    ylim(0, 7) +\n",
        "    theme_bw() + \n",
        "    theme(figure_size = (5, 5),\n",
        "          axis_line = element_line(size = 0.5, colour = \"black\"),\n",
        "          panel_grid_major = element_line(size = 0.05, colour = \"black\"),\n",
        "          panel_grid_minor = element_line(size = 0.05, colour = \"black\"),\n",
        "          axis_text = element_text(colour ='black'))\n",
        ")\n",
        "fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0ZoM8nthel5",
        "colab_type": "text"
      },
      "source": [
        "Let's split this mangnificently large dataset further to make `training` and `test` sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1DL2VV5N-td",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "example_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PK07UsaZhlgn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df = example_data.iloc[[0,2,3],:] # select 1st, 3rd and 4th samples into the training set\n",
        "print(train_df)\n",
        "\n",
        "test_df = example_data.iloc[[1,4],:] # select 2nd and 5th samples into the test set\n",
        "print(test_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRXHucmFh7cT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = (\n",
        "    ggplot(data = train_df,\n",
        "          mapping = aes(x = 'x', y = 'y')) +\n",
        "    geom_point(fill = '#36B059', \n",
        "               size = 5.0,\n",
        "               stroke = 2.5,\n",
        "               colour = '#2BE062',\n",
        "               shape = 'o') +\n",
        "    labs(\n",
        "        title ='',\n",
        "        x = 'X',\n",
        "        y = 'y',\n",
        "    ) +\n",
        "    xlim(0, 6) +\n",
        "    ylim(0, 7) +\n",
        "    theme_bw() + \n",
        "    theme(figure_size = (5, 5),\n",
        "          axis_line = element_line(size = 0.5, colour = \"black\"),\n",
        "          panel_grid_major = element_line(size = 0.05, colour = \"black\"),\n",
        "          panel_grid_minor = element_line(size = 0.05, colour = \"black\"),\n",
        "          axis_text = element_text(colour ='black'))\n",
        ")\n",
        "\n",
        "fig = fig + geom_point(data = test_df,\n",
        "          mapping = aes(x = 'x', y = 'y'), fill = 'blue', \n",
        "               size = 5.0,\n",
        "               stroke = 2.5,\n",
        "               colour = 'lightblue',\n",
        "               shape = 'o')\n",
        "fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NfQLaMZ36I2",
        "colab_type": "text"
      },
      "source": [
        "Now that data is ready let's train three linear regression models: basic linear regression, ridge regression and lasso regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eE3KiNLvDtl-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
        "\n",
        "# Initialising all three regression models\n",
        "lr = LinearRegression()\n",
        "\n",
        "lambd = 1 # you can use lambda_\n",
        "\n",
        "# Ridge regression (template)\n",
        "lr_ridge = Ridge(lambd)\n",
        "\n",
        "# Lasso \n",
        "lr_lasso = Lasso(lambd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwgQWpZG4sUL",
        "colab_type": "text"
      },
      "source": [
        "We fit all three models on our improvised training data (`train_df`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sq2wk-RLEXTN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr.fit(train_df[['x']], train_df[['y']])\n",
        "lr_ridge.fit(train_df[['x']], train_df[['y']])\n",
        "lr_lasso.fit(train_df[['x']], train_df[['y']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZsSXIpaE22m",
        "colab_type": "text"
      },
      "source": [
        "Visualise all three lines on one plot:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Gj_eL1vE2Tm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = fig + geom_abline(intercept = lr.intercept_, slope = lr.coef_[0], linetype=\"dashed\", size=1)\n",
        "fig = fig + geom_abline(intercept = lr_ridge.intercept_, slope = lr_ridge.coef_[0], color=\"red\", linetype=\"solid\", size=1)\n",
        "fig = fig + geom_abline(intercept = lr_lasso.intercept_, slope = lr_lasso.coef_[0], color=\"blue\", linetype=\"solid\", size=1)\n",
        "fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLkPcByV5A1Z",
        "colab_type": "text"
      },
      "source": [
        "What can you say about this plot? How well each model performs? On train? On test?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQbDwFIL7I94",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# predicting test set by each of the models\n",
        "lr_pred = lr.predict(test_df[['x']])\n",
        "lr_ridge_pred = lr_ridge.predict(test_df[['x']])\n",
        "lr_lasso_pred = lr_lasso.predict(test_df[['x']]).reshape((2,1)) # we need to reshape the resulting vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32Hw21AISW3K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(lr_pred.shape)\n",
        "print(lr_ridge_pred.shape)\n",
        "print(lr_lasso_pred.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lcq0N9Kaivts",
        "colab_type": "text"
      },
      "source": [
        "Here we add a function that computes Residual Sum of Squares"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLWtwcBrtibt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rss(predicted, true): # RSS == Residual Sum of Squares\n",
        "  return(np.sum((true - predicted)**2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pVKq-6pizBh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# compute RSS for each model \n",
        "lr_rss = rss(lr_pred, test_df[['y']])\n",
        "lr_ridge_rss = rss(lr_ridge_pred, test_df[['y']])\n",
        "lr_lasso_rss = rss(lr_lasso_pred, test_df[['y']])\n",
        "\n",
        "print(f'RSS for Linear Regression: {np.array(lr_rss)}')\n",
        "print(f'RSS for Ridge Regression: {np.array(lr_ridge_rss)}')\n",
        "print(f'RSS for Lasso Regression: {np.array(lr_lasso_rss)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xQEjFvq6ai1",
        "colab_type": "text"
      },
      "source": [
        "Let's now combine predictions of three regression models into an ensemble by averaging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hg_RwkC7nXEe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ensemble_preds = np.mean([lr_pred, lr_ridge_pred, lr_lasso_pred], axis = 0)\n",
        "print(ensemble_preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FH22UB2vnTY7",
        "colab_type": "text"
      },
      "source": [
        "What about RSS of averaged ensemble? What would be an expected value?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCmUyqt1n-7r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(np.mean([lr_rss, lr_ridge_rss, lr_lasso_rss]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmWk6vlqpqdz",
        "colab_type": "text"
      },
      "source": [
        "What do we actually get?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpKaagyQn77c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ensemble_rss = rss(ensemble_preds, test_df[['y']])\n",
        "print(np.array(ensemble_rss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6uubRD08OmB",
        "colab_type": "text"
      },
      "source": [
        "How do we visualise resulting model?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UY8se1BY8Q9W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We will create a dummy data that will be projected onto the ensemble that will help us visualise its predictions\n",
        "background_data = pd.DataFrame({'x': np.linspace(start=0, stop=6, num=61)})\n",
        "background_data['ensemble_y'] = np.mean((lr.predict(background_data[['x']]), lr_ridge.predict(background_data[['x']]), lr_lasso.predict(background_data[['x']]).reshape((61,1))), axis = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyqnlIjs8k0W",
        "colab_type": "text"
      },
      "source": [
        "Let's make our ensemble purple (mature colour)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JT24y1S5oN1M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig + geom_path(data = background_data, mapping = aes(x = 'x', y = 'ensemble_y'), size = 1.5, colour = 'purple') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIaVjQzP4xSs",
        "colab_type": "text"
      },
      "source": [
        "# Basic ensemble via majority vote (in case of classification)\n",
        "In classification majority vote is used when predictions of different models are merged into an ensemble. But first we shall generate some synthetic data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcQtWhUL4zTP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(2342347823) # random seed, this number was random, no need to make conspiracies around it\n",
        "\n",
        "D = 2 # two dimensions\n",
        "N = 100 # points per class\n",
        "\n",
        "# Generating N points for the first class\n",
        "mu_vec1 = np.zeros(D) # creates a vector of zeros, these are averages across each dimension\n",
        "cov_mat1 = np.eye(D) # creates a diagonal matrix of size D x D, all values except diagonal are 0\n",
        "class1_sample = np.random.multivariate_normal(mu_vec1, cov_mat1, N)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BC5EUz6R7zf3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The same stuff as above, just averages are shifted into 1\n",
        "mu_vec2 = np.ones(D) # creates a vector of ones\n",
        "cov_mat2 = np.eye(D)\n",
        "class2_sample = np.random.multivariate_normal(mu_vec2, cov_mat2, N)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S66hxBw542tn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# a lot of boring things....\n",
        "# gluing together two matrices generated above\n",
        "train = np.concatenate((class1_sample, class2_sample), axis=0)\n",
        "train_data = pd.DataFrame(train)\n",
        "\n",
        "# Create names for columns, actually there are only two this time\n",
        "train_data.columns = [ 'x' + str(i) for i in (np.arange(D)+1)]\n",
        "\n",
        "# Create a class column\n",
        "train_data['class'] = np.concatenate((np.repeat(0, N), np.repeat(1, N)))\n",
        "\n",
        "# This is important for plotting and modelling\n",
        "train_data['class'] = train_data['class'].astype('category')\n",
        "\n",
        "# Randomly splitting data into train (60%) and validation (40%)\n",
        "from sklearn.model_selection import train_test_split\n",
        "train, val = train_test_split(train_data, random_state = 111, test_size = 0.40) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJ8ShPuMjsHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function that draws data points and colour them based on the class\n",
        "def draw_points_ggplot2(point_set):\n",
        "  fig = (\n",
        "    ggplot(data = point_set,\n",
        "          mapping = aes(x = 'x1', y = 'x2')) +\n",
        "    geom_point(aes(colour = 'class', \n",
        "                   shape = 'class',\n",
        "                   fill = 'class'), \n",
        "               size = 5.0,\n",
        "               stroke = 2.5) +\n",
        "    labs(\n",
        "        title ='',\n",
        "        x = 'x1',\n",
        "        y = 'x2',\n",
        "    ) +\n",
        "    theme_bw() + \n",
        "    scale_color_manual(['#EC5D57', '#51A7F9']) + \n",
        "    scale_fill_manual(['#C82506', '#0365C0']) + \n",
        "    scale_shape_manual(['o', 's']) + \n",
        "    theme(figure_size = (5, 5),\n",
        "          axis_line = element_line(size = 0.5, colour = \"black\"),\n",
        "          panel_grid_major = element_line(size = 0.05, colour = \"black\"),\n",
        "          panel_grid_minor = element_line(size = 0.05, colour = \"black\"),\n",
        "          axis_text = element_text(colour ='black'))\n",
        "  )\n",
        "  return(fig)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wi3y0nbWj1Wp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# let's test it!\n",
        "draw_points_ggplot2(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbBy3f-BM4S1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "draw_points_ggplot2(val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0VjW1KV-wrH",
        "colab_type": "text"
      },
      "source": [
        "Now we will train three different classifiers, namely DT, KNN and LogisticRegression, which is a classifier despite its name. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TF50WImi5MYS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "np.random.seed(1111) # random seed for consistency\n",
        "\n",
        "# define all three classifiers\n",
        "model1 = DecisionTreeClassifier(max_depth = 5)\n",
        "model2 = KNeighborsClassifier()\n",
        "model3 = LogisticRegression()\n",
        "\n",
        "# train classifiers\n",
        "model1.fit(train[['x1', 'x2']],train[['class']])\n",
        "model2.fit(train[['x1', 'x2']],train[['class']])\n",
        "model3.fit(train[['x1', 'x2']],train[['class']])\n",
        "\n",
        "# predict validation set\n",
        "val['model1'] = model1.predict(val[['x1', 'x2']])\n",
        "val['model2'] = model2.predict(val[['x1', 'x2']])\n",
        "val['model3'] = model3.predict(val[['x1', 'x2']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rvEsz36BAx7",
        "colab_type": "text"
      },
      "source": [
        "**Exercise** now that we have predictions from all three models, it is time to combine them using majority vote, make a new column `ensemble` in the pandas data.frame `val` with ensembled predictions from three models. \n",
        "\n",
        "Hint: what mathematical function returns the value that appears most often."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heJc_yrIAm60",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##### YOUR CODE STARTS #####\n",
        "val['ensemble'] = val[['model1', 'model2', 'model3']].mode(axis = 1)\n",
        "##### YOUR CODE ENDS #####"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmqfyGEV_ayq",
        "colab_type": "text"
      },
      "source": [
        "One handy way to compute accuracy of the `sklearn` model is to use function `score`. Each classification model has it own `score` method but in our case all of them return accuracy by default. You can use your own metric or choose one from the exhaustive list: https://scikit-learn.org/stable/modules/model_evaluation.html. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ucUtSCr7QyY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f\"Accuracy of DT {model1.score(val[['x1', 'x2']], val['class'])*100}%\")\n",
        "print(f\"Accuracy of NN {model2.score(val[['x1', 'x2']], val['class'])*100}%\")\n",
        "print(f\"Accuracy of LR {model3.score(val[['x1', 'x2']], val['class'])*100}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qz4c_naPBre4",
        "colab_type": "text"
      },
      "source": [
        "This trick will not work for ensemble (as we don't have a model object to call function `score`). Let's calculate the accuracy in old school way."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3OJfLNhAqGV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##### YOUR CODE STARTS #####\n",
        "print(f\"Accuracy of ensemble of DT, NN and LR {(np.sum(val['ensemble'] == val['class'])/len(val))*100}%\")\n",
        "##### YOUR CODE ENDS #####"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yh3rqy7D9M7f",
        "colab_type": "text"
      },
      "source": [
        "Let's visualise decision boundaries of three classifiers and the ensemble."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSL14-8etKNm",
        "colab_type": "text"
      },
      "source": [
        "The following function generates a synthetic 2D point grid, that spans from `start` to `stop` along `x1` and `x2` dimension. You should be able to specify the number of points per unit of distance, e.g. if there would be only one dimension (e.g. `x1`) that would span from 0 (`start`) to 2 (`stop`) with 3 points per unit (`ppu`) of distance you would need to create a vector `[0, 0.4, 0.8, 1.2, 1.6, 2.0]`. You can create this output using function `np.linspace(start=0, stop=2, num=3*(2+0))`. Now you should do this for 2D."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YS6t4EIBbXO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_grid(start, stop, ppu):\n",
        "  num_points = (np.abs(start) + np.abs(stop))*ppu\n",
        "  grid_data = pd.concat([pd.DataFrame({'x1': np.repeat(x, num_points), \n",
        "                                       'x2': np.linspace(start=start, stop=stop, num=num_points)}) for x in np.linspace(start=start, stop=stop, num=num_points)])\n",
        "  return(grid_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Wj6KBmF9Lyo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start = -3 \n",
        "stop = 4\n",
        "ppu = 20 # points per unit\n",
        "\n",
        "grid_data = generate_grid(start, stop, ppu)\n",
        "print(grid_data.shape) # it should be (num_points squared, 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgSwd9teFm-v",
        "colab_type": "text"
      },
      "source": [
        "Now that you have the grid, predict each point of this grid by each of our models, including the ensemble:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCHnJP4NugDl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##### YOUR CODE STARTS #####\n",
        "grid_data['model1'] = model1.predict(grid_data[['x1', 'x2']])\n",
        "grid_data['model2'] = model2.predict(grid_data[['x1', 'x2']])\n",
        "grid_data['model3'] = model3.predict(grid_data[['x1', 'x2']])\n",
        "grid_data['ensemble'] = grid_data[['model1', 'model2', 'model3']].mode(axis = 1)\n",
        "##### YOUR CODE ENDS #####"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfaxAvefF46z",
        "colab_type": "text"
      },
      "source": [
        "We are ready to visualise each model covered with its respecting decision area"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9t3qhz4My-e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "draw_points_ggplot2(val) + geom_point(data = grid_data, mapping = aes(x = 'x1', y = 'x2', colour = 'factor(model1)'),  size = .5, alpha = 0.2) + annotate(\"text\", label = \"DecisionTree\", x = 2.8, y = 3.5, size = 12, colour = \"black\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djkw2LM4BSFh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "draw_points_ggplot2(val) + geom_point(data = grid_data, mapping = aes(x = 'x1', y = 'x2', colour = 'factor(model2)'),  size = .5, alpha = 0.2) + annotate(\"text\", label = \"K-NN\", x = 2.8, y = 3.5, size = 12, colour = \"black\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnlAODBIBdMT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "draw_points_ggplot2(val) + geom_point(data = grid_data, mapping = aes(x = 'x1', y = 'x2', colour = 'factor(model3)'),  size = .5, alpha = 0.2) + annotate(\"text\", label = \"LogisticRegression\", x = 2.5, y = 3.5, size = 12, colour = \"black\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOKUj3rQEFNn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "draw_points_ggplot2(val) + geom_point(data = grid_data, mapping = aes(x = 'x1', y = 'x2', colour = 'factor(ensemble)'),  size = .5, alpha = 0.2) + annotate(\"text\", label = \"Ensemble\", x = 2.8, y = 3.5, size = 12, colour = \"black\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YdXOODXHTVj",
        "colab_type": "text"
      },
      "source": [
        "`VotingClassifier` function from  `sklearn`, implements simple ensemble using different classifiers. Let's see how it works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYogqInelyMb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import our guest\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "np.random.seed(1111) # nothing interesting here, read on\n",
        "\n",
        "# Specify correct estimators/classifiers\n",
        "ensemble_model = VotingClassifier(estimators=[('dt', model1), ('knn', model2), ('lr', model3)], voting='hard')\n",
        "\n",
        "# Train the VotingClassifier model on training data \n",
        "ensemble_model.fit(train[['x1', 'x2']],train[['class']])\n",
        "\n",
        "# Predict validation data using trained model\n",
        "val['ensemble'] = ensemble_model.predict(val[['x1', 'x2']])\n",
        "\n",
        "# Use score function to evaluate VotingClassifier's performance\n",
        "print(f\"Accuracy of sklearn ensemble {ensemble_model.score(val[['x1', 'x2']], val[['class']])*100}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uJsm18JILnm",
        "colab_type": "text"
      },
      "source": [
        "To remind ourselves the accurace of our hand made ensemble:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qT8nQt7EkOLA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f\"Accuracy of ensemble of DT, NN and LR {np.mean(val['ensemble'] == val['class'])*100}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fY16gH4fQs39",
        "colab_type": "text"
      },
      "source": [
        "## Weighted ensemble (Classification)\n",
        "As we learnt in the lecture sometimes we prefer to trust some classifiers more than others and this is reflected in the way how ensembles are constructed. Here we will use MNIST dataset to test weighted ensembling approach. \n",
        "\n",
        "In the meantime, some setup code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ea47XUCVbP1r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# old school TF\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR) # credit to Dmitry Lekhovitsky\n",
        "\n",
        "# MNIST lives here:\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=False)\n",
        "\n",
        "images = np.vstack([img.reshape(-1,) for img in mnist.train.images])\n",
        "labels = mnist.train.labels\n",
        "print(f\"images are of shape: {images.shape} and labels: {labels.shape}\")\n",
        "\n",
        "train_images = images[0:2000,:]\n",
        "train_labels = labels[0:2000]\n",
        "\n",
        "test_images = images[2000:3000,:]\n",
        "test_labels = labels[2000:3000]\n",
        "\n",
        "train_images = pd.DataFrame(np.matrix(train_images))\n",
        "test_images = pd.DataFrame(np.matrix(test_images))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BN9XIhQ1boWg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "# initialize model templates\n",
        "model1 = DecisionTreeClassifier()\n",
        "model2 = KNeighborsClassifier()\n",
        "model3 = LogisticRegression()\n",
        "model = VotingClassifier(estimators=[('dt', model1), ('knn', model2), ('lr', model3)], voting='hard')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5pB6-5YcuMk",
        "colab_type": "text"
      },
      "source": [
        "Train each model on the training images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vskb3k7jctLR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(1111) \n",
        "model1.fit(train_images, train_labels)\n",
        "\n",
        "np.random.seed(1111) \n",
        "model2.fit(train_images, train_labels)\n",
        "\n",
        "np.random.seed(1111) \n",
        "model3.fit(train_images, train_labels)\n",
        "\n",
        "np.random.seed(1111) \n",
        "model.fit(train_images, train_labels)\n",
        "print('done')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEnLTPvcczbu",
        "colab_type": "text"
      },
      "source": [
        "Here we predit classes for test images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBr73h_ucNqw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model1_pred = model1.predict(test_images)\n",
        "model2_pred = model2.predict(test_images)\n",
        "model3_pred = model3.predict(test_images)\n",
        "ensemble_pred = model.predict(test_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsJv-ICKKrHH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f\"Accuracy of DT {model1.score(test_images, test_labels)*100}%\")\n",
        "print(f\"Accuracy of NN {model2.score(test_images, test_labels)*100}%\")\n",
        "print(f\"Accuracy of LR {model3.score(test_images, test_labels)*100}%\")\n",
        "print(f\"Accuracy of ensemble {model.score(test_images, test_labels)*100}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXhESp1rLDmz",
        "colab_type": "text"
      },
      "source": [
        "87.6% is not the greatest performance. Let's see if we can improve it.\n",
        "`VotingClassifier` has a parameter `weights` which specifies the \"level of trust\" that we have in each of the models, higher the weight more we trusth the model. Let's replicate the basic ensemble using parameter `weights`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GOQCG03j_Db",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(1111) \n",
        "\n",
        "##### YOUR CODE STARTS #####\n",
        "# set equal weights for each of the classifiers to reproduce the basic majority vote ensemble:\n",
        "model = VotingClassifier(estimators=[('dt', model1), ('knn', model2), ('lr', model3)], voting='hard', weights = [0.3,0.3,0.3])\n",
        "##### YOUR CODE ENDS #####\n",
        "\n",
        "model.fit(train_images, train_labels)\n",
        "print(f\"Accuracy of ensemble {model.score(test_images, test_labels)*100}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utyIUT6yMPlw",
        "colab_type": "text"
      },
      "source": [
        "Now let's change the weights. As we saw in the lecture model's performance across CV iterations seems to be a reasonable ground for estimating the trust we have in model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjHwRJK9NVry",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "Let's use a `cross_val_score` function from `sklearn`!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiSYnhCVbr4G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "X = np.array(train_images)\n",
        "y = np.array(train_labels)\n",
        "\n",
        "scores_model1 = cross_val_score(model1, X, y, cv=4)\n",
        "print(f'Average validation accuracy for model1 is {np.mean(scores_model1)}')\n",
        "\n",
        "scores_model2 = cross_val_score(model2, X, y, cv=4)\n",
        "print(f'Average validation accuracy for model2 is {np.mean(scores_model2)}')\n",
        "\n",
        "scores_model3 = cross_val_score(model3, X, y, cv=4)\n",
        "print(f'Average validation accuracy for model3 is {np.mean(scores_model3)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "limD6KZ1PbXf",
        "colab_type": "text"
      },
      "source": [
        "**NB!** Keep in mind that `cross_val_score` does not shuffle your data, you can pass `StratifiedKFold` object as a value for `cv` parameter. This `StratifiedKFold` object should be created from your data using option `shuffle=True`. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5VBbq6jTsB_",
        "colab_type": "text"
      },
      "source": [
        "Now we can use these scores to infer model weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7P3LIyh2k5d5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(1111) \n",
        "\n",
        "##### YOUR CODE STARTS #####\n",
        "model = VotingClassifier(estimators=[('dt', model1), ('knn', model2), ('lr', model3)], voting='hard', weights = [np.mean(scores_model1), np.mean(scores_model2), np.mean(scores_model3)])\n",
        "##### YOUR CODE ENDS ##### (please do not delete this line)\n",
        "\n",
        "# Train a new ensemble\n",
        "model.fit(train_images, train_labels)\n",
        "\n",
        "# Evaluate it's performance on the test images\n",
        "print(f\"Accuracy of ensemble {model.score(test_images, test_labels)*100}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVYv7HkcnM6d",
        "colab_type": "text"
      },
      "source": [
        "# Bagging (**B**ootstrap + **AGG**regation = **BAGG**ing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JO3ByHzAnWdx",
        "colab_type": "text"
      },
      "source": [
        "## Bootstrap (1st step)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HspFJTBjajqW",
        "colab_type": "text"
      },
      "source": [
        "Here is the familiar decision tree model we have built before:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxK7N3h2nOXC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "np.random.seed(1111) # random seed for consistency\n",
        "\n",
        "model1 = DecisionTreeClassifier()\n",
        "model1.fit(train[['x1', 'x2']],train[['class']])\n",
        "print(f\"Accuracy of a signle DT {model1.score(val[['x1', 'x2']], val[['class']])*100}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bi8eVqYcRdk",
        "colab_type": "text"
      },
      "source": [
        "Let's bootstrap 3 equal random parts of training data. \n",
        "\n",
        "**NB!** What is the difference between using `np.random.seed = 1111` as a separate command and `random_state = 1111` inside resample function?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rL8ebtdbaHjX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.utils import resample\n",
        "n_bootstraps = 3\n",
        "np.random.seed(1111)\n",
        "\n",
        "# from StackOverFlow\n",
        "resamples = [resample(train[['x1', 'x2']], n_samples = int(train[['x1', 'x2']].shape[0]*0.8)).index.values for i in range(n_bootstraps)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RonyL8r1zzwX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# first resample\n",
        "train_resample1 = train.loc[resamples[0]]\n",
        "\n",
        "# second resample\n",
        "train_resample2 = train.loc[resamples[1]]\n",
        "\n",
        "# third resample\n",
        "train_resample3 = train.loc[resamples[2]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ile_wHNKxS2V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "draw_points_ggplot2(train_resample1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HC6EW77qxT5L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "draw_points_ggplot2(train_resample2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2kKIwXvsNE5",
        "colab_type": "text"
      },
      "source": [
        "Let's train **3** identical DTs on each resample."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pt_L9GLqmQ0c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We couldn't use only one variable as\n",
        "# we wouldn't be able to capture progress of each DT independently\n",
        "model1 = DecisionTreeClassifier()\n",
        "model2 = DecisionTreeClassifier()\n",
        "model3 = DecisionTreeClassifier()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kJq1159zybK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(1111) # random seed for consistency\n",
        "\n",
        "# Note, that we cannot use VotingClassifier as before, \n",
        "# as each tree has to be trained on its own data\n",
        "model1.fit(train_resample1[['x1','x2']], train_resample1[['class']])\n",
        "model2.fit(train_resample2[['x1','x2']], train_resample2[['class']])\n",
        "model3.fit(train_resample3[['x1','x2']], train_resample3[['class']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bj9O686hs0eF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start = -3 \n",
        "stop = 4\n",
        "ppu = 20 # points per unit\n",
        "\n",
        "grid_data = generate_grid(start, stop, ppu)\n",
        "\n",
        "grid_data['model1'] = model1.predict(grid_data[['x1', 'x2']])\n",
        "grid_data['model2'] = model2.predict(grid_data[['x1', 'x2']])\n",
        "grid_data['model3'] = model3.predict(grid_data[['x1', 'x2']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0zNbXBgiqBr",
        "colab_type": "text"
      },
      "source": [
        "Let's visualise these resamples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpKV4hyGismo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "draw_points_ggplot2(train_resample1) + geom_point(data = grid_data, mapping = aes(x = 'x1', y = 'x2', colour = 'factor(model1)'),  size = .5, alpha = 0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gf7D0KFTjRS8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "draw_points_ggplot2(train_resample2) + geom_point(data = grid_data, mapping = aes(x = 'x1', y = 'x2', colour = 'factor(model2)'),  size = .5, alpha = 0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiltiUz1jUOF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "draw_points_ggplot2(train_resample3) + geom_point(data = grid_data, mapping = aes(x = 'x1', y = 'x2', colour = 'factor(model3)'),  size = .5, alpha = 0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxmR3GBdnd6w",
        "colab_type": "text"
      },
      "source": [
        "## Aggregation (2nd step)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wy0zDYII-KMr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "draw_points_ggplot2(val) + geom_point(data = grid_data, mapping = aes(x = 'x1', y = 'x2', colour = 'factor(model3)'),  size = .5, alpha = 0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAdtph3K-F7D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val['model1'] = model1.predict(val[['x1', 'x2']])\n",
        "val['model2'] = model2.predict(val[['x1', 'x2']])\n",
        "val['model3'] = model3.predict(val[['x1', 'x2']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvfEw07PoD0a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val['bagg_ensemble'] = val[['model1', 'model2', 'model3']].mode(axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ccca1o-mpDMi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f\"Accuracy of hand made bagged ensemble with 3 DTs {np.sum(val['bagg_ensemble'] == val['class'])/len(val[['class']])*100}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quRVCufixajq",
        "colab_type": "text"
      },
      "source": [
        "# Bagging in sklearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxcWuqIyyX37",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# In sklearn, there is also BaggingRegressor as might have imagined\n",
        "# BaggingClassifier is called Bagging meta-estimator\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "\n",
        "# Base classifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# humdrum random seed thingy (aka piano in the bushes)\n",
        "np.random.seed(1111)\n",
        "\n",
        "bagger = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=3, max_samples=0.8)\n",
        "\n",
        "# Train bagger\n",
        "bagger.fit(train[['x1','x2']], train['class'])\n",
        "\n",
        "print(f\"Accuracy of sklearn bagging with {3} DTs {bagger.score(val[['x1', 'x2']], val[['class']])*100}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDcMMbnWzOQy",
        "colab_type": "text"
      },
      "source": [
        "What if we try more estimators?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGPkhW0Pmpop",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialise our bagging classifier that consists of 9 DTs\n",
        "bagger = BaggingClassifier(base_estimator=DecisionTreeClassifier(),\n",
        "                        n_estimators=3, max_samples=0.8, random_state = 1).fit(train[['x1','x2']], train['class'])\n",
        "\n",
        "print(f\"Accuracy of sklearn bagging with {11} DTs {bagger.score(val[['x1', 'x2']], val[['class']])*100}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4G-jQRef3je",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Random Forest algorithm\n",
        "I would call Random Forest - the working horse of ML. Here we will not implement the Random Forest algorithm, but we will get very close to its understanding.\n",
        "\n",
        "First, let's regenerate some data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhzRyO7ywleS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(2342347823) # random seed for consistency\n",
        "\n",
        "D = 50\n",
        "N = 50\n",
        "\n",
        "# Generating 50 points for the first class\n",
        "mu_vec1 = np.zeros(D) \n",
        "cov_mat1 = np.eye(D) # creates a diagonal matrix of size D x D, all values except diagonal are 0\n",
        "class1_sample = np.random.multivariate_normal(mu_vec1, cov_mat1, N)\n",
        "\n",
        "# Generating 50 points for the second class\n",
        "mu_vec2 = np.ones(D)\n",
        "cov_mat2 = np.eye(D)\n",
        "class2_sample = np.random.multivariate_normal(mu_vec2, cov_mat2, N)\n",
        "\n",
        "train = np.concatenate((class1_sample, class2_sample), axis=0)\n",
        "train_data = pd.DataFrame(train)\n",
        "\n",
        "# Create names for columns, x1, x2 ... x50\n",
        "train_data.columns = [ 'x' + str(i) for i in (np.arange(D)+1)]\n",
        "\n",
        "# Create a class column\n",
        "train_data['class'] = np.concatenate((np.repeat(0, N), np.repeat(1, N)))\n",
        "\n",
        "# This is important for plotting and modelling\n",
        "train_data['class'] = train_data['class'].astype('category')\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "train, val = train_test_split(train_data, random_state = 111, test_size = 0.40) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67EgfC1n4kg2",
        "colab_type": "text"
      },
      "source": [
        "Regular `DecisionTree` will suffer from the curse of dimensionality with this high-dimensional data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tCx4ZYm4jqr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "model = DecisionTreeClassifier() \n",
        "\n",
        "np.random.seed(1111)\n",
        "# 2D data\n",
        "model.fit(train[['x1','x2']], train['class'])\n",
        "print(f\"Validation accuracy is {model.score(val[['x1','x2']], val[['class']])*100}%\")\n",
        "\n",
        "# 5D data\n",
        "model.fit(train.iloc[:, :5], train['class'])\n",
        "print(f\"Validation accuracy is {model.score(val.iloc[:, :5], val[['class']])*100}%\")\n",
        "\n",
        "# 50D data\n",
        "model.fit(train.iloc[:, :50], train['class'])\n",
        "print(f\"Validation accuracy is {model.score(val.iloc[:, :50], val[['class']])*100}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFya6LjjXU3y",
        "colab_type": "text"
      },
      "source": [
        "More stable estimates can be obtained using `cross_val_score` function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCbLUsWQ_Sdt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = DecisionTreeClassifier() \n",
        "\n",
        "np.random.seed(1111)\n",
        "\n",
        "# as `cross_val_score` does not shuffle the data by itself\n",
        "shuffled_train_data = train_data.sample(frac=1)\n",
        "\n",
        "# 2D data\n",
        "scores_model1 = cross_val_score(model, shuffled_train_data[['x1','x2']], shuffled_train_data['class'], cv=4)\n",
        "print(f'Average validation accuracy on 2D is {np.mean(scores_model1)*100}%')\n",
        "\n",
        "# 5D data\n",
        "scores_model2 = cross_val_score(model, shuffled_train_data.iloc[:, :5], shuffled_train_data['class'], cv=4)\n",
        "print(f'Average validation accuracy on 5D is {np.mean(scores_model2)*100}%')\n",
        "\n",
        "# 50D data\n",
        "scores_model3 = cross_val_score(model, shuffled_train_data.iloc[:, :50], shuffled_train_data['class'], cv=4)\n",
        "print(f'Average validation accuracy on 50D is {np.mean(scores_model3)*100}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjnBSC_D8kNL",
        "colab_type": "text"
      },
      "source": [
        "Not a fair comparison, because DTs are single trees, while random forest is a bagging classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnjgqTbNB0QR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bagger = BaggingClassifier(base_estimator=DecisionTreeClassifier(), max_samples=0.8, n_estimators=9, random_state=1111)\n",
        "\n",
        "# 2D data\n",
        "scores_model1 = cross_val_score(bagger, shuffled_train_data[['x1','x2']], shuffled_train_data['class'], cv=4)\n",
        "print(f'Average validation accuracy on 2D is {np.mean(scores_model1)*100}%')\n",
        "\n",
        "# 5D data\n",
        "scores_model2 = cross_val_score(bagger, shuffled_train_data.iloc[:, :5], shuffled_train_data['class'], cv=4)\n",
        "print(f'Average validation accuracy on 5D is {np.mean(scores_model2)*100}%')\n",
        "\n",
        "# 50D data\n",
        "scores_model3 = cross_val_score(bagger, shuffled_train_data.iloc[:, :50], shuffled_train_data['class'], cv=4)\n",
        "print(f'Average validation accuracy on 50D is {np.mean(scores_model3)*100}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5XO1-SCZ2Ug",
        "colab_type": "text"
      },
      "source": [
        "To make another step from the bag of decision trees to random forest, we can set a value for `max_features` parameter to something that is less than 1.0 (e.g. 0.8). This would ensure that every tree in the bag receives a random set of initial features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKtb_X6sZeTG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bagger = BaggingClassifier(base_estimator=DecisionTreeClassifier(), max_samples = 0.8, max_features = 0.8, n_estimators=9, random_state=1111)\n",
        "\n",
        "# 2D data\n",
        "scores_model1 = cross_val_score(bagger, shuffled_train_data[['x1','x2']], shuffled_train_data['class'], cv=4)\n",
        "print(f'Average validation accuracy on 2D is {np.mean(scores_model1)*100}%')\n",
        "\n",
        "# 5D data\n",
        "scores_model2 = cross_val_score(bagger, shuffled_train_data.iloc[:, :5], shuffled_train_data['class'], cv=4)\n",
        "print(f'Average validation accuracy on 5D is {np.mean(scores_model2)*100}%')\n",
        "\n",
        "# 50D data\n",
        "scores_model3 = cross_val_score(bagger, shuffled_train_data.iloc[:, :50], shuffled_train_data['class'], cv=4)\n",
        "print(f'Average validation accuracy on 50D is {np.mean(scores_model3)*100}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YV5vE-YUabYO",
        "colab_type": "text"
      },
      "source": [
        "Finally, let's train the random classifier itself."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HGiew8sXvUE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "model = RandomForestClassifier()\n",
        "\n",
        "np.random.seed(1111)\n",
        "\n",
        "# 2D data\n",
        "scores_model1 = cross_val_score(model, shuffled_train_data[['x1','x2']], shuffled_train_data['class'], cv=4)\n",
        "print(f'Average validation accuracy for model1 is {np.mean(scores_model1)*100}%')\n",
        "\n",
        "# 5D data\n",
        "scores_model2 = cross_val_score(model, shuffled_train_data.iloc[:, :5], shuffled_train_data['class'], cv=4)\n",
        "print(f'Average validation accuracy for model1 is {np.mean(scores_model2)*100}%')\n",
        "\n",
        "# 50D data\n",
        "scores_model3 = cross_val_score(model, shuffled_train_data.iloc[:, :50], shuffled_train_data['class'], cv=4)\n",
        "print(f'Average validation accuracy for model1 is {np.mean(scores_model3)*100}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6ZTyju_-wQU",
        "colab_type": "text"
      },
      "source": [
        "Extremely Randomized Trees (extreme RF)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tsqRUQQER9h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "model = ExtraTreesClassifier()\n",
        "\n",
        "np.random.seed(1111)\n",
        "\n",
        "# 2D data\n",
        "scores_model1 = cross_val_score(model, shuffled_train_data[['x1','x2']], shuffled_train_data['class'], cv=4)\n",
        "print(f'Average validation accuracy for model1 is {np.mean(scores_model1)*100}%')\n",
        "\n",
        "# 5D data\n",
        "scores_model2 = cross_val_score(model, shuffled_train_data.iloc[:, :5], shuffled_train_data['class'], cv=4)\n",
        "print(f'Average validation accuracy for model1 is {np.mean(scores_model2)*100}%')\n",
        "\n",
        "# 50D data\n",
        "scores_model3 = cross_val_score(model, shuffled_train_data.iloc[:, :50], shuffled_train_data['class'], cv=4)\n",
        "print(f'Average validation accuracy for model1 is {np.mean(scores_model3)*100}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaNAK1aN_cQR",
        "colab_type": "text"
      },
      "source": [
        "# Setup before the part II"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxoby8Xl_cpe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# For plotting like a pro\n",
        "!pip install plotnine\n",
        "from plotnine import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6_Wgv87_t4K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function that draws data points and colour them based on the class\n",
        "def draw_points_ggplot2(point_set):\n",
        "  fig = (\n",
        "    ggplot(data = point_set,\n",
        "          mapping = aes(x = 'x1', y = 'x2')) +\n",
        "    geom_point(aes(colour = 'class', \n",
        "                   shape = 'class',\n",
        "                   fill = 'class'), \n",
        "               size = 5.0,\n",
        "               stroke = 2.5) +\n",
        "    labs(\n",
        "        title ='',\n",
        "        x = 'x1',\n",
        "        y = 'x2',\n",
        "    ) +\n",
        "    theme_bw() + \n",
        "    scale_color_manual(['#EC5D57', '#51A7F9']) + \n",
        "    scale_fill_manual(['#C82506', '#0365C0']) + \n",
        "    scale_shape_manual(['o', 's']) + \n",
        "    theme(figure_size = (5, 5),\n",
        "          axis_line = element_line(size = 0.5, colour = \"black\"),\n",
        "          panel_grid_major = element_line(size = 0.05, colour = \"black\"),\n",
        "          panel_grid_minor = element_line(size = 0.05, colour = \"black\"),\n",
        "          axis_text = element_text(colour ='black'))\n",
        "  )\n",
        "  return(fig)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8f9mtSQBYcG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_grid(start, stop, ppu):\n",
        "  num_points = (np.abs(start) + np.abs(stop))*ppu\n",
        "  grid_data = pd.concat([pd.DataFrame({'x1': np.repeat(x, num_points), \n",
        "                                       'x2': np.linspace(start=start, stop=stop, num=num_points)}) for x in np.linspace(start=start, stop=stop, num=num_points)])\n",
        "  return(grid_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JS76Sg2J_4eV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start = 0 \n",
        "stop = 6\n",
        "ppu = 20 # points per unit\n",
        "\n",
        "grid_data = generate_grid(start, stop, ppu)\n",
        "grid_data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qK5__WIFVZNe",
        "colab_type": "text"
      },
      "source": [
        "# Boosting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TTCc-l7VawF",
        "colab_type": "text"
      },
      "source": [
        "## Adaptive boosting (Adaboost)\n",
        "We shall build decision stumps (decision trees of depth 1) on the toy data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OydMnXyj313",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "example_data = pd.DataFrame({'x1':[1,2,3,4,5], 'x2':[2,4,5,4,5], 'class':[1,0,1,1,0]})\n",
        "example_data['class'] = example_data['class'].astype('category') # note that we turn class into categories\n",
        "draw_points_ggplot2(example_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49KoN0Dad5jX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "dt = DecisionTreeClassifier()\n",
        "dt.fit(example_data[['x1', 'x2']], example_data[['class']])\n",
        "\n",
        "start = 0 \n",
        "stop = 6\n",
        "ppu = 20 # points per unit\n",
        "\n",
        "grid_data = generate_grid(start, stop, ppu)\n",
        "grid_data['dt'] = dt.predict(grid_data[['x1', 'x2']])\n",
        "\n",
        "# visualise the initial egalitarian tree\n",
        "draw_points_ggplot2(example_data) + geom_point(data = grid_data, mapping = aes(x = 'x1', y = 'x2', colour = 'factor(dt)'),  size = .5, alpha = 0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuaMoOH5k_np",
        "colab_type": "text"
      },
      "source": [
        "Let's build the first decision stump:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umu0X6YtLzqC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "model1 = DecisionTreeClassifier(max_depth=1) # remember that it can only have 1 level"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEgt1s-JL02m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "initial_weights = np.ones(len(example_data)) # egalitarian world of samples\n",
        "print(initial_weights)\n",
        "\n",
        "np.random.seed(1111)\n",
        "\n",
        "model1.fit(example_data[['x1', 'x2']], example_data[['class']], sample_weight = initial_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5sc8csTL2op",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start = 0 \n",
        "stop = 6\n",
        "ppu = 20 # points per unit\n",
        "\n",
        "grid_data = generate_grid(start, stop, ppu)\n",
        "grid_data['model1'] = model1.predict(grid_data[['x1', 'x2']])\n",
        "\n",
        "# visualise the initial egalitarian tree\n",
        "draw_points_ggplot2(example_data) + geom_point(data = grid_data, mapping = aes(x = 'x1', y = 'x2', colour = 'factor(model1)'),  size = .5, alpha = 0.2) + geom_text(aes(label = initial_weights), nudge_y = 0.4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBww79t5T2-R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model1.predict(example_data[['x1', 'x2']]) != example_data['class']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffquXIklMHA0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "incorrect = model1.predict(example_data[['x1', 'x2']]) != example_data['class']\n",
        "print(np.array(incorrect))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfCgPGy_u7hn",
        "colab_type": "text"
      },
      "source": [
        "**Exercise** update the weights as discussed in the lecture (add score of 0.5 to those points that were misclassified and remove 0.5 from classified correctly) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAWxqwbvMJic",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy\n",
        "new_weights = copy.deepcopy(initial_weights)\n",
        "##### YOUR CODE STARTS #####\n",
        "new_weights[np.array(~incorrect)] = \n",
        "new_weights[np.array(incorrect)] = \n",
        "##### YOUR CODE ENDS #####\n",
        "\n",
        "print(new_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLNsFs15ZiJC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "draw_points_ggplot2(example_data) + geom_text(aes(label = new_weights), nudge_y = 0.2) + geom_point(data = grid_data, mapping = aes(x = 'x1', y = 'x2', colour = 'factor(model1)'),  size = .5, alpha = 0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2y4lfNljMkLr",
        "colab_type": "text"
      },
      "source": [
        "**Exercise** Repeat the same process for models #2 and #3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8uzpb9Z06Pd",
        "colab_type": "text"
      },
      "source": [
        "Let's build the second tree using these new weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2INUYU7KMv-F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(1111)\n",
        "##### YOUR CODE STARTS #####\n",
        "model2 = \n",
        "model2.fit\n",
        "##### YOUR CODE ENDS #####"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLpVsSppM3D-",
        "colab_type": "text"
      },
      "source": [
        "Visualising boundaries of the second tree:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEwv2EVJM2H8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##### YOUR CODE STARTS #####\n",
        "grid_data['model2'] = \n",
        "##### YOUR CODE ENDS #####\n",
        "draw_points_ggplot2(example_data) + geom_point(data = grid_data, mapping = aes(x = 'x1', y = 'x2', colour = 'factor(model2)'),  size = .5, alpha = 0.2) + geom_text(aes(label = new_weights), nudge_y = 0.4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1oGSJTLM9JN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##### YOUR CODE STARTS #####\n",
        "incorrect = \n",
        "##### YOUR CODE ENDS #####\n",
        "\n",
        "print(np.array(incorrect))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrP8nbGTM6KP",
        "colab_type": "text"
      },
      "source": [
        "Changing the weights for the second time:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8leWy1KNDhH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##### YOUR CODE STARTS #####\n",
        "newer_weights = copy.deepcopy(new_weights)\n",
        "newer_weights[np.array(incorrect)] = \n",
        "newer_weights[np.array(~incorrect)] = \n",
        "##### YOUR CODE ENDS #####\n",
        "print(newer_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OL32wWeXNH0i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(1111)\n",
        "\n",
        "##### YOUR CODE STARTS #####\n",
        "model3 = \n",
        "model3.fit\n",
        "##### YOUR CODE ENDS #####"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKZ2ggPeDDWJ",
        "colab_type": "text"
      },
      "source": [
        "Visualising the decision boundaries of the third tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TESrrpW-NNxj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##### YOUR CODE STARTS #####\n",
        "grid_data['model3'] = \n",
        "##### YOUR CODE ENDS #####\n",
        "draw_points_ggplot2(example_data) + geom_point(data = grid_data, mapping = aes(x = 'x1', y = 'x2', colour = 'factor(model3)'),  size = .5, alpha = 0.2) + geom_text(aes(label = newer_weights), nudge_y = 0.4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kss9kah3DHzL",
        "colab_type": "text"
      },
      "source": [
        "Putting all these trees together into one model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ek7T_yc-o8e6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid_data['ensemble'] = grid_data[['model1', 'model2', 'model3']].mode(axis = 1)\n",
        "draw_points_ggplot2(example_data) + geom_point(data = grid_data, mapping = aes(x = 'x1', y = 'x2', colour = 'factor(ensemble)'),  size = .5, alpha = 0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_AJru_wDN6O",
        "colab_type": "text"
      },
      "source": [
        "Let's compare to the official `AdaBoostClassifier` implmentation from the `sklearn`. Pay attention to the parameters, we want 3 models, with each one of them being `DecisionTreeClassifier` with `max_depth = 1`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9Lps8N-pcbX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "model = AdaBoostClassifier(n_estimators=3, base_estimator=DecisionTreeClassifier(max_depth=1), random_state=1)\n",
        "\n",
        "# train AdaBoost on our data\n",
        "model.fit(example_data[['x1','x2']], example_data[['class']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bSy0MN9FeAp",
        "colab_type": "text"
      },
      "source": [
        "Here we visualise AdaBoost decision boundaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9qgaxtuFdYy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid_data['ada_ensemble'] = model.predict(grid_data[['x1', 'x2']])\n",
        "draw_points_ggplot2(example_data) + geom_point(data = grid_data, mapping = aes(x = 'x1', y = 'x2', colour = 'factor(ada_ensemble)'),  size = .5, alpha = 0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nuZ40omVcYQ",
        "colab_type": "text"
      },
      "source": [
        "## Gradient boosting machines (GBM)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNlzsyRpPLZq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "example_data = pd.DataFrame({'x1':[1,2,3,4,5], 'x2':[2,4,5,4,5], 'class':[1,0,1,1,0]})\n",
        "# note that now we actually don't turn \"class\" into categorical\n",
        "# we will treat this problem as regreession now"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnq_uLKplb6b",
        "colab_type": "text"
      },
      "source": [
        "Fit the first **`DecisionTreeRegressor`** model on the original data. I have not found any restrictions on the size of the tree for the gradient boosting algorithm, but let's keep decision stumps as before.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISbv7SCPlYre",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "model1 = DecisionTreeRegressor(max_depth=1) # let's keep 1 level trees\n",
        "\n",
        "np.random.seed(111)\n",
        "\n",
        "model1.fit(example_data[['x1', 'x2']], example_data[['class']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JuEANqLmIFJ",
        "colab_type": "text"
      },
      "source": [
        "Now, let's predict the data using this first tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDfDCALSmHrH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions_model1 = model1.predict(example_data[['x1','x2']])\n",
        "print(f'predictions of the first tree: {predictions_model1}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2bDLbFVmgVe",
        "colab_type": "text"
      },
      "source": [
        "Find the residuals (subtract predictions from the ground truth)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mgDtIvwmosv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "errors_model1 = example_data['class'] - predictions_model1\n",
        "print(f'residuals: {errors_model1}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdmrhCJ8m4An",
        "colab_type": "text"
      },
      "source": [
        "Now use these errors as a `target` for the second tree!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXi6A5A7m8Z-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(1111)\n",
        "\n",
        "model2 = DecisionTreeRegressor(max_depth=1)\n",
        "model2.fit(X = example_data[['x1', 'x2']], y = errors_model1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaVqXMnxQmln",
        "colab_type": "text"
      },
      "source": [
        "**Exercise** implement the same procedure for the second and third models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgpxGSaYnCiF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##### YOUR CODE STARTS #####\n",
        "predictions_model2 = \n",
        "##### YOUR CODE ENDS #####\n",
        "print(f'predictions of the second tree: {predictions_model2}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hf63TOw3nLGP",
        "colab_type": "text"
      },
      "source": [
        "Add these to the predictions obtained by the first model. Subtract the resulting sum from the ground truth."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95c2Et57nPZR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##### YOUR CODE STARTS #####\n",
        "errors_model2 = \n",
        "##### YOUR CODE ENDS #####\n",
        "print(f'residuals: {errors_model2}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zURJnd2WninU",
        "colab_type": "text"
      },
      "source": [
        "Do the same for the last third tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DR8DWnj9nhwp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(1111)\n",
        "\n",
        "##### YOUR CODE STARTS #####\n",
        "model3 = \n",
        "model3.fit\n",
        "##### YOUR CODE ENDS #####"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4q-8xV5hn0T0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##### YOUR CODE STARTS #####\n",
        "predictions_model3 = \n",
        "##### YOUR CODE ENDS #####\n",
        "print(f'predictions of the first tree: {predictions_model3}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJ9jGUGDnzA0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##### YOUR CODE STARTS #####\n",
        "errors_model3 = \n",
        "##### YOUR CODE ENDS #####\n",
        "print(f'residuals: {errors_model3}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dcxVH4upD_K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid_data['gbm'] = model1.predict(grid_data[['x1', 'x2']]) + model2.predict(grid_data[['x1', 'x2']]) + model3.predict(grid_data[['x1', 'x2']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVLMLfemsHUb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = (\n",
        "    ggplot(data = grid_data,\n",
        "          mapping = aes(x = 'x1', y = 'x2')) +\n",
        "    geom_point(aes(colour = 'gbm'), \n",
        "               size = 1.0) +\n",
        "    labs(\n",
        "        title ='',\n",
        "        x = 'x1',\n",
        "        y = 'x2',\n",
        "    ) +\n",
        "    theme_bw() + \n",
        "    theme(figure_size = (5, 5),\n",
        "          axis_line = element_line(size = 0.5, colour = \"black\"),\n",
        "          panel_grid_major = element_line(size = 0.05, colour = \"black\"),\n",
        "          panel_grid_minor = element_line(size = 0.05, colour = \"black\"),\n",
        "          axis_text = element_text(colour ='black'))\n",
        "  )\n",
        "fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kG0n1_qpiLo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid_data.loc[grid_data['gbm'] < 0.5, 'gbm'] = 0\n",
        "grid_data.loc[grid_data['gbm'] >= 0.5, 'gbm'] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6tVb7YJp1LU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "example_data['class'] = example_data['class'].astype('category') # now we can cast `class` back into categorical\n",
        "draw_points_ggplot2(example_data) + geom_point(data = grid_data, mapping = aes(x = 'x1', y = 'x2', colour = 'factor(gbm)'),  size = .5, alpha = 0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9-z0cD9Sxxi",
        "colab_type": "text"
      },
      "source": [
        "### Gradient Boosting from sklearn (compare the results)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_p4gygbRcZO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "gbm = GradientBoostingClassifier(n_estimators=3, random_state=1) # uses DecisionTreeRegressor by default\n",
        "\n",
        "# train GBM on our data\n",
        "gbm.fit(example_data[['x1','x2']], example_data[['class']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFbc-uH5SCs1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid_data['gbm_ensemble'] = model.predict(grid_data[['x1', 'x2']])\n",
        "draw_points_ggplot2(example_data) + geom_point(data = grid_data, mapping = aes(x = 'x1', y = 'x2', colour = 'factor(gbm_ensemble)'),  size = .5, alpha = 0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gezm0AO80ary",
        "colab_type": "text"
      },
      "source": [
        "## Homework exercise 1: eXtreme Gradient Boosting (XGBoost)\n",
        "\n",
        "\n",
        "<font color='red'> Let's finally build for ourselves a new shiny XGBoost model, the most popular algorithm for Kaggle competitions. </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gauRESFyT03k",
        "colab_type": "text"
      },
      "source": [
        "<font color='red'> First, we need to load data (we shall use MNIST data again). </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-ZTcrSLPFkm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# old school TF\n",
        "%tensorflow_version 1.x\n",
        "\n",
        "# MNIST lives here:\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "# Downloading MNIST from tensorflow into MNIST_data/ folder\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=False)\n",
        "\n",
        "# Extracting individual images and labels\n",
        "images = np.vstack([img.reshape(-1,) for img in mnist.train.images])\n",
        "labels = mnist.train.labels\n",
        "\n",
        "# Split into train and test as before\n",
        "train_images = images[0:2000,:]\n",
        "train_labels = labels[0:2000]\n",
        "\n",
        "test_images = images[2000:3000,:]\n",
        "test_labels = labels[2000:3000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKc_JzT5UN0n",
        "colab_type": "text"
      },
      "source": [
        "<font color='red'> **(a)** Use the tutorial page (https://xgboost.readthedocs.io/en/latest/python/python_intro.html and https://www.kaggle.com/anktplwl91/mnist-xgboost) to fill in the gaps in the following code and traing the XGBoost model. **(3 points)** </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzsOrKP0ENjV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "##### YOUR CODE STARTS #####\n",
        "\n",
        "# XGBoosts wants data to be wrapped into special formats\n",
        "dtrain = \n",
        "dtest = \n",
        "\n",
        "# most meaningful parameters\n",
        "param_list = [(\"objective\", \"multi:softmax\"), (\"eval_metric\", \"merror\"), (\"num_class\", 10)]\n",
        "\n",
        "# Number of trees\n",
        "n_rounds = 600\n",
        "\n",
        "# if nothing seems to improve for 50 iterations - stop\n",
        "early_stopping = 50\n",
        "\n",
        "# train for training and test for ... validation!    \n",
        "eval_list = \n",
        "\n",
        "# 1,2,3.. go!\n",
        "bst = xgb.train(param_list, dtrain, n_rounds, evals=eval_list, early_stopping_rounds=early_stopping, verbose_eval=True)\n",
        "##### YOUR CODE ENDS #####"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a27j0swxUi4D",
        "colab_type": "text"
      },
      "source": [
        "<font color='red'> **(b)** Use the same tutorial page (https://xgboost.readthedocs.io/en/latest/python/python_intro.html) to find out how to evaluate the model **(1 point)** </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8fHF6FtRJJL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##### YOUR CODE STARTS #####\n",
        "\n",
        "##### YOUR CODE ENDS #####"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dQ4Q2EnU7ur",
        "colab_type": "text"
      },
      "source": [
        "<font color='red'> Are you impressed with XGBoost performance? </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JK3GFrVcVAcA",
        "colab_type": "text"
      },
      "source": [
        "<font color='red'> **(c)** Train a simple KNN model from sklearn (KNeighborsClassifier) on the same trainign data and evaluate on the same validation data **(2 points)** </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5yjj2PsTsLF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn = \n",
        "knn.fit\n",
        "knn.score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5PHU3XgVR2C",
        "colab_type": "text"
      },
      "source": [
        "<font color='red'> How these two models compare? </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9FfQTIPYAyG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write your comment here:\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubkJ0b1aYBaY",
        "colab_type": "text"
      },
      "source": [
        "<font color='red'> **(d)** gain additional 2 bonus points if you can improve XGBoost's performance by at least 5% without changing the model parameters. **(2 bonus points)** </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWHItYLrXoQE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##### YOUR CODE STARTS #####\n",
        "# Do something here to improve XGBoost by 5%\n",
        "\n",
        "##### YOUR CODE ENDS #####"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7AYje7sZK8Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##### YOUR CODE STARTS #####\n",
        "# copy your solution to (a) here:\n",
        "\n",
        "##### YOUR CODE ENDS #####"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9K8bItbIX7Ck",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##### YOUR CODE STARTS #####\n",
        "# evaluate your XGBoost model as before\n",
        "# it should be better than before\n",
        "\n",
        "##### YOUR CODE ENDS #####"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTu05w96VfG_",
        "colab_type": "text"
      },
      "source": [
        "# Stacking\n",
        "On top of everything we have seen, you can still improve the results by training the meta-learner (meta-model) that would use predictions of other models as input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUWyqvAVlswR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# old school TF\n",
        "%tensorflow_version 1.x\n",
        "\n",
        "# MNIST lives here:\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "# Downloading MNIST from tensorflow into MNIST_data/ folder\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=False)\n",
        "\n",
        "# Extracting individual images and labels\n",
        "images = np.vstack([img.reshape(-1,) for img in mnist.train.images])\n",
        "labels = mnist.train.labels\n",
        "\n",
        "# Split into train and test as before\n",
        "train_images = images[0:2000,:]\n",
        "train_labels = labels[0:2000]\n",
        "\n",
        "test_images = images[2000:3000,:]\n",
        "test_labels = labels[2000:3000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPUR4grj5fFX",
        "colab_type": "text"
      },
      "source": [
        "First we should again train familiar three models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXP9KOfFhOCy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "model1 = DecisionTreeClassifier()\n",
        "model2 = KNeighborsClassifier()\n",
        "model3 = LogisticRegression()\n",
        "\n",
        "np.random.seed(1111) \n",
        "model1.fit(train_images, train_labels)\n",
        "\n",
        "np.random.seed(1111) \n",
        "model2.fit(train_images, train_labels)\n",
        "\n",
        "np.random.seed(1111) \n",
        "model3.fit(train_images, train_labels)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3PB4PXc5lA9",
        "colab_type": "text"
      },
      "source": [
        "We can go ahead and test these machine learning models on the test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZJGqVixqWxC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model1_pred = model1.predict(test_images)\n",
        "model2_pred = model2.predict(test_images)\n",
        "model3_pred = model3.predict(test_images)\n",
        "\n",
        "print(f\"Accuracy of DT {model1.score(test_images, test_labels)*100}%\")\n",
        "print(f\"Accuracy of NN {model2.score(test_images, test_labels)*100}%\")\n",
        "print(f\"Accuracy of LR {model3.score(test_images, test_labels)*100}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrgEk0z5qeSP",
        "colab_type": "text"
      },
      "source": [
        "Classical stacking, we need to run CV algorithm and record predictions made by each model on the hold out data. Then we will use theses predictions as training data for the meta-learner."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fi09ljg2qi1u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "hold_out_pred_model1 = []\n",
        "hold_out_pred_model2 = []\n",
        "hold_out_pred_model3 = []\n",
        "\n",
        "n_folds = 4\n",
        "\n",
        "X = np.array(train_images)\n",
        "y = np.array(train_labels)\n",
        "\n",
        "# initialise splitting mechanism\n",
        "folds = StratifiedKFold(n_splits=n_folds, shuffle = False, random_state=111) # no need to shuffle the data\n",
        "\n",
        "# here actual splitting is done\n",
        "folds.get_n_splits(X, y)\n",
        "\n",
        "fold_indx = 1\n",
        "\n",
        "# folds.split is an iterator that loops over different folds\n",
        "# returning a tuple with train and val indeces\n",
        "for train_index, val_index in folds.split(X, y):\n",
        "  print(f\"CV #{fold_indx}\")\n",
        "  X_train, X_val = X[train_index], X[val_index]\n",
        "  y_train, y_val = y[train_index], y[val_index]\n",
        "\n",
        "  # train all three models\n",
        "  model1.fit(X_train, y_train)\n",
        "  model2.fit(X_train, y_train)\n",
        "  model3.fit(X_train, y_train)\n",
        "\n",
        "  # make predictions on hold out set\n",
        "  hold_out_pred_model1.append(model1.predict_proba(X_val)) # we use predict_proba function to get a vector of probabilities for each class\n",
        "  hold_out_pred_model2.append(model2.predict_proba(X_val))\n",
        "  hold_out_pred_model3.append(model3.predict_proba(X_val))\n",
        "\n",
        "  fold_indx += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTbhnWQHuCKS",
        "colab_type": "text"
      },
      "source": [
        "Let's concatenate all these predictions into one dataset. Each model outputs probabilities for each class (there are 10 classes in the dataset), which means that for each digit (2000 in the training data) we will have 10 values from each model, which adds up to 30 values in total (from 3 models). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbL53qe3uH8g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_stacking = np.concatenate([np.concatenate(hold_out_pred_model1, axis = 0), \n",
        "                                 np.concatenate(hold_out_pred_model2, axis = 0), \n",
        "                                 np.concatenate(hold_out_pred_model3, axis = 0)], \n",
        "                                axis = 1)\n",
        "train_stacking.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNMocFWcu7dm",
        "colab_type": "text"
      },
      "source": [
        "We need also a test set for the stacking model, but this is simpler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAS3x6tSvBib",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model1_pred = model1.predict_proba(test_images)\n",
        "model2_pred = model2.predict_proba(test_images)\n",
        "model3_pred = model3.predict_proba(test_images)\n",
        "\n",
        "test_stacking = np.concatenate([model1_pred, \n",
        "                                model2_pred, \n",
        "                                model3_pred], \n",
        "                                axis = 1)\n",
        "\n",
        "test_stacking.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6avJgP6udsl",
        "colab_type": "text"
      },
      "source": [
        "Train another model (e.g. LogisticRegression or DecisionTree or something else) on these predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHMy_9ucusWA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "stacking_model = SVC()\n",
        "\n",
        "np.random.seed(1111) \n",
        "stacking_model.fit(train_stacking, train_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibBzsdOBvZjj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f\"Accuracy of stacking ensemble {stacking_model.score(test_stacking, test_labels)*100}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avCryKaDzJqn",
        "colab_type": "text"
      },
      "source": [
        "## Homework exercise 2: implement blending approach\n",
        "<font color='red'> In this exercise you will practice using blending approach to meta-learning. </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGjcSfjL9HfR",
        "colab_type": "text"
      },
      "source": [
        "<font color='red'> **(a)** to implement blending we first need to create a separate validation set that would be independent from training and test data. Below, use images from 0 to 1500 as training data, images from 1500 to 2000 as validation and from 2000 to 3000 as a test set. **(1 point)** </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFLaLzggzOAX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##### YOUR CODE STARTS #####\n",
        "train_images = \n",
        "train_labels = \n",
        "\n",
        "val_images = \n",
        "val_labels = \n",
        "\n",
        "test_images = \n",
        "test_labels = \n",
        "##### YOUR CODE ENDS #####"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58hA8_x7_Ne5",
        "colab_type": "text"
      },
      "source": [
        "<font color='red'> **(b)** Train three models (decision tree, k nearest neighbors classifier, and the logistic regression) with default parameters on the train data. **(1 point)** </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9XoXdKazZac",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##### YOUR CODE STARTS #####\n",
        "model1 = \n",
        "model2 = \n",
        "model3 = \n",
        "##### YOUR CODE ENDS #####\n",
        "\n",
        "np.random.seed(1111) \n",
        "##### YOUR CODE STARTS #####\n",
        "model1\n",
        "##### YOUR CODE ENDS #####\n",
        "\n",
        "np.random.seed(1111) \n",
        "##### YOUR CODE STARTS #####\n",
        "model2\n",
        "##### YOUR CODE ENDS #####\n",
        "\n",
        "np.random.seed(1111) \n",
        "##### YOUR CODE STARTS #####\n",
        "model3\n",
        "##### YOUR CODE ENDS #####"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0A0TGEr4_i6-",
        "colab_type": "text"
      },
      "source": [
        "<font color='red'> **(c)** Create a training set for the meta-learner by concatenating the predictions made by individual models on validation images. Hint: use function `np.concatenate` and `predict_proba` as we did for stacking. **(1 point)** </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0E2CMbbnzhxm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##### YOUR CODE STARTS #####\n",
        "train_blending = \n",
        "##### YOUR CODE ENDS #####\n",
        "\n",
        "train_blending_labels = val_labels\n",
        "train_blending.shape # if all was done correctly this shape should be (500, 30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNv2gKx4AJ08",
        "colab_type": "text"
      },
      "source": [
        "<font color='red'> **(d)** Create a test set for the meta-learner by concatenating the predictions made by each model on test images. Use the same function as in the cell above. **(1 point)** </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwpZ-Z8oz1xE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##### YOUR CODE STARTS #####\n",
        "test_blending = \n",
        "##### YOUR CODE ENDS #####\n",
        "\n",
        "test_blending.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlIlOIJ_AkoS",
        "colab_type": "text"
      },
      "source": [
        "<font color='red'> **(e)** Use SVM model as a meta-learner and train it on the `train_blending` data. **(1 point)** </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sn8MwXQrzujZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "np.random.seed(1111) \n",
        "\n",
        "##### YOUR CODE STARTS #####\n",
        "blending_model = \n",
        "blending_model\n",
        "##### YOUR CODE ENDS #####"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UV3lreeTBrPz",
        "colab_type": "text"
      },
      "source": [
        "<font color='red'> **(f)** Evaluate the performance of the blending ensemble on the test set and comment on the difference between blending and stacking.  **(1 point)** </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fmgRVmc0Bsh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##### YOUR CODE STARTS #####\n",
        "print()\n",
        "##### YOUR CODE ENDS #####"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0WiXv1WCKnq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# What is your take on the difference between blending and stacking?\n",
        "# Which one would you prefer and why?\n",
        "# Comment here:\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSswjqM0hlSi",
        "colab_type": "text"
      },
      "source": [
        "# Thank you!"
      ]
    }
  ]
}